Euclidean geometry are a set of rules laid out by Euclid that follow for geometry on a flat surface

## Security
Most dangerous form of buffer overflow is overwriting return address of function to external code.
Can be solved by having a shadow stack, i.e. two stacks, one for variables, one for compiler stuff

## Common
(sign 1bit)-(exponent 8bits)-(significand/mantissa 23bits)
1 *     2² *      0.1234

PIC (Position Independent Code) can be executed anywhere in memory by using relative addresses, e.g. shared libraries
The process of converting relative to absolute, i.e. query unresolved symbols at runtime adds a level of indirection 

Data deduplication means to remove duplicates

REST (representational state transfer) is an interface that outlines how the state of a resource is interacted with
On the web, an URL is an access point to a resource
A RESTful API will have URLs respond to CRUD requests in a standard way:
* GET example.com/users will return list of resource, i.e. all users
* POST example.com/users will create a new resource
* GET example.com/users/1 return single resource
* PUT example.com/users/1 update single resource
* DELETE example.com/users/1 delete single resource

OAuth (open authorisation) is a standard that defines a way of authorising access
OAuth offers different functionality than SSH by having the ability to 'scope' access
Typically used by RESTful services
These endpoints described in 'discovery document':
1. authorisation-server -> authorisation-code
2. authorisation-code -> access token, refresh token
3. resource-server -> resource

RFC (Request For Comments) documents contain technical specifications for Internet techologies, e.g. IP, UDP, etc.

Udp (head of line blocking) +
client server (p2p unreliable as internet path optimised for cost/closest exchange point) 
+ dedicated (peoples home Internet don't normally have high upload rates); Mix of cloud (flexible to just turn up and down, high egress bandwidth charge) and bare metal (fixed bandwidth rate set into price) 
Matchmaking,  host migration difficult as hard to measure what user has good connection,  
e.g. Whats there NAT type?

## Assembly

## Bootstrap 
UUID/GUID (universally/globally) 16 bytes. 
Typically generated by concatenating bits of MAC address and timestamp

UEFI firmware interface made to standardise interface between OS and firmware 
for purposes of booting
UEFI secure boot performs a cryptographic validation of bootloader.
Therefore, the bootloaders must be signed with trusted keys
Good for detecting malware in bootloader, yet this is a rarer attack vector as opposed to say a web browser
Also has insidious implications that could see companies restrict access to particular software

Called /dev/sd as originally for SCSI 
(small computer system interface; standards for transferring data between computers and devices) 
The preceding letter indicates the order in which it's found, e.g. /dev/sda first found 
The preceding number indicates the partition number, e.g. and /dev/sda1, /dev/sda2

UEFI use of GPT (GUID partition table) incorporates CRC to create more 
recoverable boot environment
over BIOS MBR (located in first sector of disk)
Furthermore, UEFI has more addressable memory in 64-bit mode as oppose to only 16-bit mode
Also, UEFI supports networking
The ESP (EFI system partition) will have EGI entries that point to a UUID of where to boot
one of these will be grub binary like shimx64.efi
NOTE: The bootloader is the EFI OS loader and is part of the OS that will load the kernel

ACPI interface to pass information from firmware to OS.
This firmware will have hardware information baked into it set by manufacturers 

## Formats 
Inodes store file metadata.
The metadata stored by an inode is determined by filesystem in use, 
except for filename which is never stored 
Typical metadata includes size, permissions, data pointer
NOTE: FAT32 won't store permissions, last modification time, no journaling or soft-links

A filename maps to an inode.
Therefore a directory is a mapping of filenames to inodes

A hardlink references an inode, and is therefore impervious to file name change, deletion, etc.
A softlink is to a file name

Journaling is the process of regularly writing operations that are to be performed in RAM 
to disk area of memory known as journal.
then apply these changes to disk when necessary
this overhead makes them slower, but more robust on crashes as can read journal to ascertain
whether certain operations finished performing

FAT for ESP (because FAT simple, open source, low-overhead and supported virtually everywhere)
vFAT is driver (typically for FAT32)

EXT4 for system (supports larger file sizes)
(NTFS microsoft proprietary)
Most filesystems will use a self-balancing tree to index files

## Unix
Ubuntu distro as compared to debian more user friendly.
For example, automatically includes proprietary drivers like WiFi, 
has PPAs to allow installation of 3rd party applications,
and install procedure just works.
Also updates more regularly than Debian and provides LTS, so know regular backports provided

Linux is a monolithic kernel, i.e. drivers, file system etc. are all in kernel space.
So, more efficient, not as robust to component failure
Windows uses a hybrid kernel, moving away from original microkernel due to inefficiencies
Using Ubuntu generic kernel (could also have -lowlatency etc.) to not include a lot of
modules in kernel to free up RAM usage

Xfce as default Ubuntu GNOME had bug with multiple keyboards. 
Furthermore, Xfce codebase was readable when inspecting X11 code.
In addition, Xfce automatically provided GUI shortcut creation

.deb and .rpm are binary packages. 
Annoyances arise due to specifying the specific library dependency for each distro version
Flatpaks and Snaps are containerised applications that include the specific libraries and runtimes
AppImages combine the 'shared' libraries and runtimes of Flatpaks and Snaps into a giant file. 
This file can be copied and run on any distro
If packages is being actively maintained, preferable to use .deb as faster and simpler

Linux DRM (direct rendering manager) -> X11 (display server) -> xfce (desktop environment)  
Linux ALSA (advanced linux sound architecture) -> pulseaudio (sound server) 

Fstab (File System Table) describes filesystems that should be mounted, when they should be mounted and with what options.

SystemV ABI:
rdi, rsi, rdx, rcx, r8, r9 (6 integer arguments)
xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6 (7 floating arguments)
Remaining arguments pushed right-to-left on stack
rax return and syscall number
Stack 16-byte aligned before function call 
SSE2 is baseline for x86-64, so make efficient for __m128

A premptive scheduler will swap processes based on specific criteria.
Round-robin means each process will run for a designated time slice
CFS is a premptive round-robin scheduler. 
Time slices are dynamic, computed like `((1/N) * (niceness))` 
Processes are managed using a RB-Tree. 
Therefore, cost of launching a process or a context switch is logarithmic
Kernel will have an internal tick rate that updates waiting threads.
Lowering this will increase granularity however will increase CPU time 
and hence battery time as more time spent in kernel code.
Windows scheduler uses static priorities, so one intensive process can dominate CPU

System level refers to inbetween kernel and userspace, e.g. network manager
Systemd is a collection of system binaries, e.g. udev
Primarily, systemd is a service manager
A service extends the functionality of daemons, e.g. only start after another service, restart on failure after 10s etc.
The kernel will launch systemd init service that will then bootstrap into userspace 
(hence alllowing for the aforementioned service features)

Kernel offers various methods of process isolation, e.g. chroot, cgroups etc.
(chroot cannot access files outside its designated tree)
A container will utilise one of these options provided by the kernel to acheive:
 * cannot send signals to processes outside container
 * has own networking namespace
 * resource usage limits

ELF (Executable and Linkable Format)
Header contains type, e.g. executable, dynamic, relocatable and entrypoint
A section is compile time:
* .text (code)
* .bss (uninitialised globals)
* .data (globals)
* .rodata (constants)
A segment is a memory location, e.g .dseg (data), .eseg (eeprom) and .cseg (code/program)


## Legalities
Anti-trust laws don't prevent monopolies, they prevent attempts to monopolise by 
unfair means, e.g. Microsoft browser market, Apple app store etc.

Technically, any digital work created is automatically protected by copyright. 
So, without a license, people would have to explicitly ask for permission to use

Permissive (MIT, BSD, Apache, zlib) gives users more freedom to say relicense, 
include closed source software, etc.
They generally just enforce attribution
Apache like MIT except must state what files you have changed 

Weak copyleft (LPGL) applies to files of library not your entire codebase, 
i.e. must still release your version of the library used
So, dynamic linking makes this easier for keeping your source closed
If statically linking, must make a few extra steps to ensure the LGPL parts are available, 
e.g. publish object files

Copyleft (GPL) enforces the developers usage of the code.
So, any derivative software must release whole project as GPL, i.e infects your software (and restricts choice of libraries to GPL)
Subsequently encounter more licensing restrictions.

Creative commons licenses are composed of various attributes. The default is attribution.
They are typically used in artworks, e.g. images, audio files
Other elements are optional and can be combined together, 
e.g no derivative, no financial, must share under same license.

Public domain means no license, so could claim as yours

## Performance (CPU)

## Data Structures

## Matching Pairs
A quadratically scaling solution is intuitive
However, as we know every match is unique, linearly scaling solution obtained with a hash map.
C++ STL implementation of hash tables are sets (just keys) and maps
Unordered variants are raw hash maps
Ordered use self-balancing red-black-tree yielding logarithmic time
Simplest hashing function `(x >> 4 + 12) & (size - 1)`
Important to keep in mind we are executing on a physical machine and that
Big-Oh is a 'zero-cost abstraction' world.
For example, the extra overhead of introducing a hashmap (memory allocations/copies) 
will result in this being slower for small lists (also no dynamic memory allocations in ISR)
This is why C++ STL uses hybrid introsort

## Sorting Squared Array
Quadratic insertion/bubble sort preferable for small lists
Loglinear divide-and-conquer merge/quick for medium
Linear radix for large



In practice, you can solve efficiency by leaning on existing libraries/work.
More important to write robust code with tests.
Have knowledge of these rather than memorising implementations.
You can investigate specific implementations on a per-problem basis.

Big Oh is worst case.
Amortised is average over a series of operations, i.e. worse case average over a series of operations
It's an indication of how an algorithm scales, i.e. asymptotic limiting behaviour
It operates in a zero-cost abstraction world:
e.g. memory allocations involved in small input number for linear is slower than logarithmic (why C++ STL uses hybrid introsort)
e.g. how arranged in memory; complete binary trees store in array, other trees store as list
e.g. ignores constants like in Fibonacci heaps having large constants in their amortised linear cost, so often not actually better insertions than binary heap

* Array: search O(n)
  - Heap:
    The parent node is greater/less than all its child nodes
    Binary has 2 children, binomial has many
    Binary heap used to implement priority queues to allow for logarithmic insertion/deletion (could also be implemented in say a splay tree)
    - Fibonacci Heap:
      Mergeable heap
      Consists of unioned min-binomial-heaps
      Called fibonacci as each tree of order 'n' will have at least fib('n + 2') nodes in it
* Hashing: insert O(1)
  Hashing function different for strings and integers.
  Fixed array of buckets, where the index of an element is obtained by modulo'ing hash value.
  Each bucket a linked list of slots following separate chaining collision resolution.
  Each slot will store the key. 
  The key will also contain the value of the key prior to hashing to allow for collision resolution
  - Set: just keys
      - Bloom Filter
        Probalistic set, in that it can only say if an element probably exists
        This is because the storage of different keys may overlap with one another 
        Greatly reduces storage space 
  - Hashmap
* Singly Linked List: search O(n)
  Constant time merging
  Managing first and last pointers with node next pointer
  - Queue
  - Stack
  - Skip List:
    Consists of a series of linked lists
    On creation, each list will move along the lower list and randomly select a node to copy over or skip
    Search path starts from the top of these lists in a greedy fashion, i.e. if cannot go further will move down a list
    Yields searches of linked lists in O(logn)
* Doubly Linked List: deletion O(1) 
  managing first and last pointers with node next and prev pointers
* Graph:
  Adjacency list has each vertex store a linked list of all the edges it connects to
  Therefore, better for sparse graphs than adjacency matrix
  Isomorphic means same number of vertices with same degrees
* Tree: search O(logn)
  Type of undirected unweighted connected acyclic graph. 
  Cannot have cycles
  Store standalone parent node pointer
  Child nodes stored in a doubly linked list
  Therefore, store managing first and last pointers with child node next and prev pointers 
  Complete tree has all 'h-1' levels are filled, and last level filled from left to right
  Balanced tree has each node's subtree's differing in height by no more than 1 (O(logn)) 
  Full/perfect tree has all nodes filled 
  For complete binary trees, better off storing it as an array
  In other tree structures, better of storing in nodes to account for holes, e.g. say root node only has left children
  Inverting is a notorious procedure, however demonstrates recursive solutions that arise with trees
  Specifically, inverting is swapping left and right subtrees from bottom up
  - Trie:
    Prefix matcher
    Each node will contain a flag that indicates if it's a prefix or whole
  - BST:
    Each node at most degree 2.
    Nodes of left subtree have values less than node, so definitionally require distinct values.
  - B-Tree: 
    Self balancing multiway search tree
    Nodes store a number of keys, i.e. values based on order of tree
    Order 'm' has each node with at most 'm' children, i.e. 'm' comparison points and 'm-1' keys
    If on insertion into leaf node violates order, than will create new parent node based on median comparison point.
    Used in databases to speed up disk access, e.g. with say order 500
    This is because of large number of children results in lower tree height, so less disk accesses
    So, use with huge number of items or items that are grouped together
    A 2-3-4 Tree is a B-Tree of order 4
  - Red-Black tree:
    Most popular search balanaced binary search tree implementation
    Has concept of external nodes, which are just left or right pointers that are null (so different to child node)
    Root node is black
    Each red node has black child nodes
    The black depth is same for each leaf node. It's the number of black nodes (including external node, excluding root node) 
  - AVL Tree:
    Another popular search balanaced binary search tree implementation
    Balance threshold calculated by `left-subtree-height - right-subtree-height`.
    So, if left heavy, i.e. > 1, will do a right rotation.
    Rotations change 2 node positions, and then swap around children to adhere to BST invariants
    Faster lookups than red-black as maintains balance threshold of 1.
    However results in slower insertions and removals
  - KD Tree:
    Binary tree
    Breaks up space for 'k' dimensions
    'k' represents number of elements per node
    Each depth alternates what node element the BST invariant is applied to
    This has the effect of dividing into partial spaces.
    Therefore, will have to unwind to find actual nearest neighbour
    Useful for calculating nearest neighbour for static structures that don't update frequently
  - Quadtree/Octree 
    Perform spatial partitioning of 2D or 3D space respectively
    Multiway search tree, so number of direct children is 2ⁿ (where 'n' is number of dimensions) 
  - Splay Tree
    Inbalanced binary search tree, i.e. no extra operations performed on insertion and deletion
    On a search, the accessed node is rotated to the root node. 
    This makes for recently accessed nodes to retrieved fast.
  - Fenwick tree
    Also known as binary index tree, as uses bits of each index to determine how many elements to count up
    For example, indices with 0th bit 1 will add only 1 element, indices with 1st bit 1 will add 2 elements, etc.
    Considered a tree due to this indexing, although implemented as array
    Used for range checks

## Algorithms
greedy algorithms memory efficient however not optimal, so balancing act
greedy chooses next best option from what is available at the moment

TODO: add algos course notes
dynamic programming is breaking a problem into subproblems and working way up
This involves storing the results of smaller problems and using them for results on bigger problems 

Informed search is when we have a way to estimate how far away are we from our goal, i.e. have domain knowledge

* Sorting:
  - O(n²) preferable for small lists or lists that are almost sorted
    - Insertion
      Used most often
      Working from left-to-right, develop an increasing 'sorted partition' as we move up an index 
      Each iteration, compare the element when elements to its left and swap if required
    - Selection
      Working from left-to-right as base element, find next element that is absolute minimum compared to this.
      Swap items. Repeat on next index. Less memory swaps than insertion
    - Bubble
      Working from left-to-right, compare two elements and swap if out-of-order. 
      Do so until reach end. Repeat on next index.
  - O(nlogn) divide-and-conquer (typically recursive) for medium
    - Merge
      Out-place
      Divide into sub-arrays until 1 element sized.
      Sort each sub-arrays and combine.
      Logarithmic as dividing into sub-arrays creates a binary tree structure 
    - Quick (qsort() is quick-sort)
      In-place meaning sorted items occupy same storage as original ones
      Involves pivot point, i.e. a point 
      When sorting, pivot point at end. 
      Moving from left, find element larger than pivot.
      Moving from right, find element smaller than pivot.
      Swap them. When elements overlap, place pivot back where it started.
      Can yield quadratic time if pivot point chosen poorly.
      Pivot point normally chosen via median-of-3, i.e. sort 1st/middle/last element 
    - Heap 
      First create max-heap and extract max.
      Then adjust existing max-heap, which as almost ordered, is logarithmic time.
      Repeat
  - O(n) for large
    - Radix
      Sort on element radix/base, e.g. ones unit, then tens unit etc.
      On each pass, copy elements into buckets and then copy back out
      Cost of memory operations reason for applicibility

* Search/traversing:
for each algorithm, does it work on DAG?

BFS, DFS (uninformed greedy memory efficient), Djisktra (BFS with weights), A* (Djikstra with heuristic), Knuth-Morris, Prims, Floyd-Warshall
Bellman-Ford (non-greedy), topological sort (for things with dependencies), Myers-Diff (dynamic programming),
Disjoint-Set-Union-Find (minimum spanning trees), 
Each A* node has local and global goals, i.e. how close to end result

(simple hash, simple checksum, Huffman encoding, greedy, non-greedy, dynamic programming, backtracing?)

TODO: multithreading!!!

For embedded:
* buffering techniques: zero-copy buffer (direct to peripheral buffer), circular/ring buffer, bipartite buffer
* bit-twiddling tricks: 
* Usage and problems with DMAs, DMAs with and without cache coherency
* persistent logs  
* Identify if it is HW or FW problem
* debugging techniques for various peripherals
* data structure dumps
* TLB with MMU
* state machines


TODO: understanding how to benchmark algorithm.
know about various cache misses


* **Two arrays, find matching pairs between them**
Intuitive solution with array: O(n²), O(n)
Knowing uniqueness, can obtain linear solution with hashmap: O(n), O(2n)




// remove root: swap with last, sift down (swap with minimum of child nodes)

// insert: append, walk up/sift up swapping with parent

// create: iteratively sift down on parent nodes starting from end


## OS 
in uefi:
- can set fan speed based on temperature
- other frequencies to auto; inspect ram slot details; on-board leds etc.
- set cpu overclock, e.g. 3.8GHz to 4GHz
in grub can run memtest:
- notice that having 8 sticks of ram at manufacturer recommended 3GHz does gives error in test 7, so set to 2.9GHz

mandelbrot vanilla benchmarking?
$(/usr/bin/time povray) for benchmarking
run this on side (lm-sensors): https://superuser.com/questions/25176/how-can-i-monitor-the-cpu-temperature-under-linux

ESP32 more so Harvard, as actually has IRAM (e.g. to hold ISRs? just to ensure faster access than from Flash?) and DRAM?

## Networking
event-driven/interrupt or polling for networking? 
IMPORTANT: interrupt not really possible on linux; asynchronous more means callbacks in threads
so, asynchronous for desktop; interrupt for embedded (for performance)

https://github.com/icopy-site/awesome/blob/master/docs/awesome/Awesome-Game-Networking.md?plain=1

Networking Chapter in the book Hacking The Art of Exploitation
 
Distinction between unicast (device to device), multicast (device to some devices) and broadcast (device to all devices) is at network layer 3. (the signals are all technically recieved)

## Matrices
vector math routines (obtaining cross product from column vector form)
when drawing vectors in a physical sense, 
keep in mind they are rooted at the origin (even if drawings show them across time)
whenever doing vector addition/subtraction, remember the head-to-tail rule (their direction is determined by their sign).
could also think that subtract whenever you want to 'go away' from something
dot product transpose notation useful for emulating matrix multiplication
unit circle, x = cosθ
dot product allows us to project a vector's length onto a unit vector 
dot product allows us to measure a vector on any axis system we want by setting up two unit vectors that are orthogonal to each other
simple plane equation with d=0 will be through origin (altering d shifts the plane up/down)
cross product gives vector that is orthogonal to the plane that the two original vectors lie on (length is |a|·|b|·sinθ). So, really only works in at least 3 dimensions
with units, e.g. for camera, start with arbitrary 'unit' defintion. later move onto more physical things like metres
by applying a scaling factor to direction vector, can move along it
world space coordinates. camera position is based on these. the camera will have its own axis system which we determine what it should be and then use cross product based on what we want
understanding dot product equivalence with circle equation
for multiplication of vectors, be explicit with a hadamard function
(IMPORTANT have reciprocal square root approximation which is there specifically for normalisation. 
much faster cycle count and latency than square root)

